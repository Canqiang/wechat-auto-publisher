
import re
from typing import Dict, List
import jieba
from collections import Counter
import logging

logger = logging.getLogger(__name__)

class ContentOptimizer:
    """å†…å®¹ä¼˜åŒ–æœåŠ¡"""
    
    def __init__(self):
        self.load_sensitive_words()
        self.load_seo_keywords()
    
    def load_sensitive_words(self):
        """åŠ è½½æ•æ„Ÿè¯åº“"""
        try:
            with open('data/sensitive_words.txt', 'r', encoding='utf-8') as f:
                self.sensitive_words = set(line.strip() for line in f if line.strip())
            logger.info(f"åŠ è½½æ•æ„Ÿè¯åº“: {len(self.sensitive_words)}ä¸ªè¯æ±‡")
        except FileNotFoundError:
            logger.warning("æ•æ„Ÿè¯åº“æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤æ•æ„Ÿè¯")
            self.sensitive_words = {'è¿æ³•', 'è¯ˆéª—', 'èµŒåš', 'æš´åŠ›', 'è¡€è…¥'}
        except Exception as e:
            logger.error(f"åŠ è½½æ•æ„Ÿè¯åº“å¤±è´¥: {str(e)}")
            self.sensitive_words = set()
    
    def load_seo_keywords(self):
        """åŠ è½½SEOå…³é”®è¯"""
        self.seo_keywords = {
            'tech': ['äººå·¥æ™ºèƒ½', 'AI', 'ç§‘æŠ€', 'åˆ›æ–°', 'æ•°å­—åŒ–'],
            'health': ['å¥åº·', 'å…»ç”Ÿ', 'è¿åŠ¨', 'é¥®é£Ÿ', 'åŒ»ç–—'],
            'finance': ['æŠ•èµ„', 'ç†è´¢', 'é‡‘è', 'è‚¡ç¥¨', 'åŸºé‡‘'],
            'lifestyle': ['ç”Ÿæ´»', 'æ—¶å°š', 'æ—…æ¸¸', 'ç¾é£Ÿ', 'æ–‡åŒ–']
        }
    
    def optimize_content(self, content: str, category: str = None) -> Dict:
        """ä¼˜åŒ–æ–‡ç« å†…å®¹"""
        result = {
            'optimized_content': content,
            'suggestions': [],
            'seo_score': 0,
            'readability_score': 0
        }
        
        # 1. æ•æ„Ÿè¯æ£€æµ‹å’Œæ›¿æ¢
        content = self.filter_sensitive_words(content)
        
        # 2. SEOä¼˜åŒ–
        seo_result = self.optimize_for_seo(content, category)
        result['optimized_content'] = seo_result['content']
        result['seo_score'] = seo_result['score']
        
        # 3. å¯è¯»æ€§ä¼˜åŒ–
        readability = self.improve_readability(result['optimized_content'])
        result['optimized_content'] = readability['content']
        result['readability_score'] = readability['score']
        
        # 4. æ·»åŠ äº’åŠ¨å…ƒç´ 
        result['optimized_content'] = self.add_engagement_elements(
            result['optimized_content']
        )
        
        # 5. ç”Ÿæˆæ‘˜è¦
        result['summary'] = self.generate_summary(result['optimized_content'])
        
        return result
    
    def filter_sensitive_words(self, content: str) -> str:
        """è¿‡æ»¤æ•æ„Ÿè¯"""
        for word in self.sensitive_words:
            if word in content:
                content = content.replace(word, '*' * len(word))
                logger.warning(f"å‘ç°æ•æ„Ÿè¯: {word}")
        return content
    
    def optimize_for_seo(self, content: str, category: str) -> Dict:
        """SEOä¼˜åŒ–"""
        result = {'content': content, 'score': 0}
        
        if category and category in self.seo_keywords:
            keywords = self.seo_keywords[category]
            
            # è®¡ç®—å…³é”®è¯å¯†åº¦
            word_count = len(content)
            keyword_count = sum(content.count(kw) for kw in keywords)
            density = keyword_count / word_count * 100
            
            # ä¼˜åŒ–å…³é”®è¯å¯†åº¦ï¼ˆç›®æ ‡2-3%ï¼‰
            if density < 2:
                # åœ¨é€‚å½“ä½ç½®æ’å…¥å…³é”®è¯
                for keyword in keywords[:3]:
                    if content.count(keyword) < 3:
                        # åœ¨æ®µè½å¼€å¤´æˆ–ç»“å°¾æ·»åŠ å…³é”®è¯
                        paragraphs = content.split('\n\n')
                        if paragraphs:
                            paragraphs[0] = f"{keyword}æ˜¯å½“ä»Šçš„çƒ­é—¨è¯é¢˜ã€‚{paragraphs[0]}"
                            paragraphs[-1] = f"{paragraphs[-1]}å…³æ³¨{keyword}ï¼Œè·å–æ›´å¤šç²¾å½©å†…å®¹ã€‚"
                            result['content'] = '\n\n'.join(paragraphs)
            
            result['score'] = min(100, density * 33)
        
        return result
    
    def improve_readability(self, content: str) -> Dict:
        """æé«˜å¯è¯»æ€§"""
        result = {'content': content, 'score': 0}
        
        # 1. åˆ†æ®µä¼˜åŒ–
        paragraphs = content.split('\n\n')
        optimized_paragraphs = []
        
        for para in paragraphs:
            # æ®µè½ä¸è¦å¤ªé•¿
            if len(para) > 500:
                # åœ¨åˆé€‚çš„æ ‡ç‚¹å¤„åˆ†æ®µ
                sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', para)
                new_para = []
                current = []
                current_len = 0
                
                for sent in sentences:
                    if current_len + len(sent) > 250:
                        optimized_paragraphs.append('ã€‚'.join(current) + 'ã€‚')
                        current = [sent]
                        current_len = len(sent)
                    else:
                        current.append(sent)
                        current_len += len(sent)
                
                if current:
                    optimized_paragraphs.append('ã€‚'.join(current) + 'ã€‚')
            else:
                optimized_paragraphs.append(para)
        
        # 2. æ·»åŠ å°æ ‡é¢˜
        if len(optimized_paragraphs) > 5:
            # æ¯3-4æ®µæ·»åŠ ä¸€ä¸ªå°æ ‡é¢˜
            with_headers = []
            for i, para in enumerate(optimized_paragraphs):
                if i > 0 and i % 3 == 0:
                    # æ ¹æ®å†…å®¹ç”Ÿæˆå°æ ‡é¢˜
                    header = self._generate_subheading(para)
                    with_headers.append(f"\n## {header}\n")
                with_headers.append(para)
            
            result['content'] = '\n\n'.join(with_headers)
        else:
            result['content'] = '\n\n'.join(optimized_paragraphs)
        
        # è®¡ç®—å¯è¯»æ€§åˆ†æ•°
        avg_sentence_length = len(content) / max(1, content.count('ã€‚'))
        result['score'] = min(100, 100 - abs(avg_sentence_length - 30))
        
        return result
    
    def add_engagement_elements(self, content: str) -> str:
        """æ·»åŠ äº’åŠ¨å…ƒç´ """
        # åœ¨æ–‡ç« å¼€å¤´æ·»åŠ å¼•å¯¼è¯­
        intro = "ğŸ‘‹ äº²çˆ±çš„è¯»è€…ï¼Œä»Šå¤©ä¸ºæ‚¨å¸¦æ¥ç²¾å½©å†…å®¹ï¼\n\n"
        
        # åœ¨æ–‡ç« ç»“å°¾æ·»åŠ äº’åŠ¨å¼•å¯¼
        outro = "\n\nğŸ’¬ è§‰å¾—æ–‡ç« æœ‰å¸®åŠ©å—ï¼Ÿæ¬¢è¿ç‚¹èµã€è¯„è®ºå’Œåˆ†äº«ï¼\nå…³æ³¨æˆ‘ä»¬ï¼Œè·å–æ›´å¤šä¼˜è´¨å†…å®¹ï½"
        
        # æ·»åŠ æ€è€ƒé—®é¢˜
        questions = [
            "\n\nğŸ¤” æ€è€ƒï¼šè¿™ä¸ªè§‚ç‚¹æ‚¨è®¤åŒå—ï¼Ÿ",
            "\n\nğŸ’¡ å°è´´å£«ï¼šè®°å¾—å®è·µè¿™äº›æ–¹æ³•å“¦ï¼",
            "\n\nğŸ“ ç¬”è®°ï¼šè¿™ä¸ªçŸ¥è¯†ç‚¹å€¼å¾—è®°å½•ä¸‹æ¥"
        ]
        
        # åœ¨é€‚å½“ä½ç½®æ’å…¥äº’åŠ¨å…ƒç´ 
        paragraphs = content.split('\n\n')
        if len(paragraphs) > 3:
            # åœ¨ä¸­é—´ä½ç½®æ’å…¥ä¸€ä¸ªæ€è€ƒé—®é¢˜
            mid = len(paragraphs) // 2
            paragraphs[mid] += questions[0]
        
        return intro + '\n\n'.join(paragraphs) + outro
    
    def generate_summary(self, content: str, max_length: int = 100) -> str:
        """ç”Ÿæˆæ–‡ç« æ‘˜è¦"""
        # æå–å‰å‡ å¥è¯ä½œä¸ºæ‘˜è¦
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', content)
        summary = []
        current_length = 0
        
        for sent in sentences:
            if current_length + len(sent) > max_length:
                break
            summary.append(sent)
            current_length += len(sent)
        
        return 'ã€‚'.join(summary) + 'ã€‚' if summary else content[:max_length]
    
    def _generate_subheading(self, paragraph: str) -> str:
        """æ ¹æ®æ®µè½å†…å®¹ç”Ÿæˆå°æ ‡é¢˜"""
        # ä½¿ç”¨jiebaåˆ†è¯
        words = jieba.cut(paragraph)
        word_freq = Counter(words)
        
        # è¿‡æ»¤åœç”¨è¯
        stop_words = {'çš„', 'æ˜¯', 'åœ¨', 'å’Œ', 'äº†', 'æœ‰', 'æˆ‘', 'ä½ ', 'ä»–', 'å¥¹', 'å®ƒ'}
        for word in stop_words:
            word_freq.pop(word, None)
        
        # è·å–é«˜é¢‘è¯
        top_words = word_freq.most_common(3)
        if top_words:
            return ''.join([word for word, _ in top_words[:2]])
        
        return "ç²¾å½©å†…å®¹"
    
    def check_content_quality(self, content: str) -> Dict:
        """æ£€æŸ¥å†…å®¹è´¨é‡"""
        quality_score = 100
        issues = []
        recommendations = []
        
        # 1. é•¿åº¦æ£€æŸ¥
        length = len(content)
        if length < 300:
            quality_score -= 20
            issues.append("å†…å®¹è¿‡çŸ­")
            recommendations.append("å»ºè®®å¢åŠ å†…å®¹æ·±åº¦ï¼Œè‡³å°‘300å­—")
        elif length > 5000:
            quality_score -= 10
            issues.append("å†…å®¹è¿‡é•¿")
            recommendations.append("å»ºè®®ç²¾ç®€å†…å®¹æˆ–åˆ†ä¸ºå¤šç¯‡æ–‡ç« ")
        
        # 2. æ®µè½ç»“æ„æ£€æŸ¥
        paragraphs = content.split('\n\n')
        if len(paragraphs) < 3:
            quality_score -= 15
            issues.append("æ®µè½è¿‡å°‘")
            recommendations.append("å»ºè®®å¢åŠ æ®µè½åˆ†éš”ï¼Œæå‡å¯è¯»æ€§")
        
        # 3. å¥å­é•¿åº¦æ£€æŸ¥
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', content)
        long_sentences = [s for s in sentences if len(s) > 100]
        if len(long_sentences) > len(sentences) * 0.3:
            quality_score -= 10
            issues.append("é•¿å¥è¿‡å¤š")
            recommendations.append("å»ºè®®æ‹†åˆ†é•¿å¥ï¼Œæå‡é˜…è¯»æµç•…åº¦")
        
        # 4. é‡å¤å†…å®¹æ£€æŸ¥
        words = jieba.cut(content)
        word_freq = Counter(words)
        most_common = word_freq.most_common(10)
        for word, freq in most_common:
            if len(word) > 1 and freq > length // 100:  # è¯é¢‘è¿‡é«˜
                quality_score -= 5
                issues.append(f"'{word}'è¯é¢‘è¿‡é«˜")
                recommendations.append(f"å‡å°‘'{word}'çš„ä½¿ç”¨é¢‘ç‡ï¼Œå¢åŠ è¡¨è¾¾å¤šæ ·æ€§")
        
        # 5. æ ‡ç‚¹ç¬¦å·æ£€æŸ¥
        punctuation_count = len(re.findall(r'[ã€‚ï¼ï¼Ÿï¼Œã€ï¼›ï¼š]', content))
        if punctuation_count < length // 50:
            quality_score -= 10
            issues.append("æ ‡ç‚¹ç¬¦å·è¿‡å°‘")
            recommendations.append("é€‚å½“å¢åŠ æ ‡ç‚¹ç¬¦å·ï¼Œæå‡é˜…è¯»èŠ‚å¥")
        
        return {
            'quality_score': max(0, quality_score),
            'issues': issues,
            'recommendations': recommendations,
            'word_count': length,
            'paragraph_count': len(paragraphs),
            'sentence_count': len(sentences)
        }
    
    def enhance_readability(self, content: str) -> Dict:
        """æå‡å†…å®¹å¯è¯»æ€§"""
        enhanced_content = content
        improvements = []
        
        # 1. æ·»åŠ è¡¨æƒ…ç¬¦å·
        enhanced_content = self._add_emojis(enhanced_content)
        improvements.append("æ·»åŠ äº†ç›¸å…³è¡¨æƒ…ç¬¦å·")
        
        # 2. ä¼˜åŒ–æ®µè½ç»“æ„
        enhanced_content = self._optimize_paragraphs(enhanced_content)
        improvements.append("ä¼˜åŒ–äº†æ®µè½ç»“æ„")
        
        # 3. æ·»åŠ å¼ºè°ƒæ ¼å¼
        enhanced_content = self._add_emphasis(enhanced_content)
        improvements.append("æ·»åŠ äº†æ–‡æœ¬å¼ºè°ƒ")
        
        # 4. æ’å…¥é˜…è¯»æç¤º
        enhanced_content = self._add_reading_hints(enhanced_content)
        improvements.append("æ’å…¥äº†é˜…è¯»æç¤º")
        
        return {
            'enhanced_content': enhanced_content,
            'improvements': improvements,
            'readability_score': self._calculate_readability_score(enhanced_content)
        }
    
    def _add_emojis(self, content: str) -> str:
        """æ·»åŠ ç›¸å…³è¡¨æƒ…ç¬¦å·"""
        emoji_map = {
            'é‡è¦': 'âš ï¸',
            'æ³¨æ„': 'âš ï¸',
            'æé†’': 'ğŸ’¡',
            'å»ºè®®': 'ğŸ’¡',
            'æŠ€å·§': 'ğŸ’¡',
            'æ–¹æ³•': 'ğŸ”§',
            'æ­¥éª¤': 'ğŸ“',
            'æ€»ç»“': 'ğŸ“‹',
            'ç»“è®º': 'âœ…',
            'æˆåŠŸ': 'âœ…',
            'å¤±è´¥': 'âŒ',
            'é—®é¢˜': 'â“',
            'ç–‘é—®': 'â“',
            'æ€è€ƒ': 'ğŸ¤”',
            'åˆ†æ': 'ğŸ“Š',
            'æ•°æ®': 'ğŸ“Š',
            'è¶‹åŠ¿': 'ğŸ“ˆ',
            'å¢é•¿': 'ğŸ“ˆ',
            'ä¸‹é™': 'ğŸ“‰',
            'æ—¶é—´': 'â°',
            'é‡‘é’±': 'ğŸ’°',
            'æŠ•èµ„': 'ğŸ’°',
            'å­¦ä¹ ': 'ğŸ“š',
            'æ•™è‚²': 'ğŸ“š',
            'å¥åº·': 'ğŸ¥',
            'è¿åŠ¨': 'ğŸƒ',
            'é£Ÿç‰©': 'ğŸ',
            'ç§‘æŠ€': 'ğŸ’»',
            'æœªæ¥': 'ğŸš€'
        }
        
        for keyword, emoji in emoji_map.items():
            if keyword in content and emoji not in content:
                # åœ¨å…³é”®è¯å‰æ·»åŠ è¡¨æƒ…ç¬¦å·ï¼Œä½†ä¸è¦è¿‡å¤š
                content = content.replace(keyword, f"{emoji} {keyword}", 1)
        
        return content
    
    def _optimize_paragraphs(self, content: str) -> str:
        """ä¼˜åŒ–æ®µè½ç»“æ„"""
        paragraphs = content.split('\n\n')
        optimized = []
        
        for para in paragraphs:
            # å¦‚æœæ®µè½å¤ªé•¿ï¼Œå°è¯•åˆ†å‰²
            if len(para) > 400:
                sentences = re.split(r'([ã€‚ï¼ï¼Ÿ])', para)
                current_para = ""
                
                for i in range(0, len(sentences), 2):
                    if i + 1 < len(sentences):
                        sentence = sentences[i] + sentences[i + 1]
                        if len(current_para + sentence) > 200 and current_para:
                            optimized.append(current_para.strip())
                            current_para = sentence
                        else:
                            current_para += sentence
                
                if current_para:
                    optimized.append(current_para.strip())
            else:
                optimized.append(para)
        
        return '\n\n'.join(optimized)
    
    def _add_emphasis(self, content: str) -> str:
        """æ·»åŠ æ–‡æœ¬å¼ºè°ƒ"""
        # ä¸ºé‡è¦æ¦‚å¿µæ·»åŠ åŠ ç²—
        important_patterns = [
            r'(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]\w*)',
            r'(\d+[ä¸ªæœˆå¹´å¤©å°æ—¶åˆ†é’Ÿ])',
            r'([0-9]+%)',
            r'(å…³é”®æ˜¯|é‡ç‚¹æ˜¯|æ ¸å¿ƒæ˜¯)',
            r'(æ€»ç»“|ç»“è®º|è¦ç‚¹)'
        ]
        
        for pattern in important_patterns:
            content = re.sub(pattern, r'**\1**', content)
        
        return content
    
    def _add_reading_hints(self, content: str) -> str:
        """æ·»åŠ é˜…è¯»æç¤º"""
        hints = [
            "\n\nğŸ’¡ **å°è´´å£«**: ä»¥ä¸‹å†…å®¹æ¯”è¾ƒé‡è¦ï¼Œå»ºè®®ä»”ç»†é˜…è¯»ã€‚\n\n",
            "\n\nğŸ“ **åˆ’é‡ç‚¹**: è¿™é‡Œçš„ä¿¡æ¯å€¼å¾—è®°å½•ä¸‹æ¥ã€‚\n\n",
            "\n\nğŸ¤” **æ€è€ƒä¸€ä¸‹**: è¿™ä¸ªè§‚ç‚¹æ‚¨æ€ä¹ˆçœ‹ï¼Ÿ\n\n"
        ]
        
        paragraphs = content.split('\n\n')
        if len(paragraphs) > 4:
            # åœ¨ä¸­é—´ä½ç½®æ’å…¥æç¤º
            mid = len(paragraphs) // 2
            paragraphs.insert(mid, hints[0])
        
        return '\n\n'.join(paragraphs)
    
    def _calculate_readability_score(self, content: str) -> int:
        """è®¡ç®—å¯è¯»æ€§åˆ†æ•°"""
        score = 50  # åŸºç¡€åˆ†
        
        # æ®µè½æ•°é‡
        paragraphs = len(content.split('\n\n'))
        if 3 <= paragraphs <= 8:
            score += 10
        
        # å¹³å‡å¥å­é•¿åº¦
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', content)
        if sentences:
            avg_sentence_length = len(content) / len(sentences)
            if 15 <= avg_sentence_length <= 35:
                score += 10
        
        # è¡¨æƒ…ç¬¦å·ä½¿ç”¨
        emoji_count = len(re.findall(r'[ğŸ¯ğŸ’¡ğŸ“ğŸ¤”ğŸ“ŠğŸ“ˆğŸ“‰â°ğŸ’°ğŸ“šğŸ¥ğŸƒğŸğŸ’»ğŸš€âš ï¸âœ…âŒâ“]', content))
        if emoji_count > 0:
            score += min(emoji_count * 2, 15)
        
        # å¼ºè°ƒæ–‡æœ¬
        emphasis_count = len(re.findall(r'\*\*.*?\*\*', content))
        if emphasis_count > 0:
            score += min(emphasis_count * 3, 15)
        
        return min(100, score)
    
    def generate_tags(self, content: str, max_tags: int = 10) -> List[str]:
        """è‡ªåŠ¨ç”Ÿæˆå†…å®¹æ ‡ç­¾"""
        # ä½¿ç”¨jiebaæå–å…³é”®è¯
        words = jieba.cut(content)
        word_freq = Counter(words)
        
        # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
        stop_words = {
            'çš„', 'æ˜¯', 'åœ¨', 'å’Œ', 'äº†', 'æœ‰', 'æˆ‘', 'ä½ ', 'ä»–', 'å¥¹', 'å®ƒ',
            'è¿™', 'é‚£', 'ä¸', 'åŠ', 'æˆ–', 'ä½†', 'è€Œ', 'å°±', 'éƒ½', 'å¾ˆ',
            'ä¹Ÿ', 'è¿˜', 'åª', 'åˆ', 'æŠŠ', 'è¢«', 'è®©', 'ä»', 'åˆ°', 'ä¸º',
            'ä¸€ä¸ª', 'ä¸€äº›', 'è¿™ä¸ª', 'é‚£ä¸ª', 'å¯ä»¥', 'åº”è¯¥', 'éœ€è¦', 'èƒ½å¤Ÿ'
        }
        
        # æå–å€™é€‰æ ‡ç­¾
        candidate_tags = []
        for word, freq in word_freq.items():
            if (len(word) >= 2 and 
                word not in stop_words and 
                not word.isdigit() and
                freq >= 2):
                candidate_tags.append((word, freq))
        
        # æ’åºå¹¶è¿”å›å‰Nä¸ª
        candidate_tags.sort(key=lambda x: x[1], reverse=True)
        tags = [tag for tag, _ in candidate_tags[:max_tags]]
        
        # å¦‚æœæ ‡ç­¾ä¸å¤Ÿï¼Œæ·»åŠ ä¸€äº›é€šç”¨æ ‡ç­¾
        if len(tags) < 3:
            generic_tags = ['å®ç”¨æŠ€å·§', 'ç”Ÿæ´»æŒ‡å—', 'å¹²è´§åˆ†äº«', 'ç»éªŒæ€»ç»“']
            for tag in generic_tags:
                if tag not in tags:
                    tags.append(tag)
                    if len(tags) >= max_tags:
                        break
        
        return tags
    
    def optimize_for_platform(self, content: str, platform: str = 'wechat') -> Dict:
        """é’ˆå¯¹ç‰¹å®šå¹³å°ä¼˜åŒ–å†…å®¹"""
        if platform == 'wechat':
            return self._optimize_for_wechat(content)
        elif platform == 'weibo':
            return self._optimize_for_weibo(content)
        elif platform == 'zhihu':
            return self._optimize_for_zhihu(content)
        else:
            return {'optimized_content': content, 'changes': []}
    
    def _optimize_for_wechat(self, content: str) -> Dict:
        """å¾®ä¿¡å…¬ä¼—å·å†…å®¹ä¼˜åŒ–"""
        optimized = content
        changes = []
        
        # 1. æ·»åŠ å¼•å¯¼å…³æ³¨
        if 'å…³æ³¨' not in optimized:
            optimized += "\n\nğŸ”” è§‰å¾—å†…å®¹æœ‰ç”¨ï¼Ÿåˆ«å¿˜äº†ç‚¹èµå’Œå…³æ³¨æˆ‘ä»¬ï¼"
            changes.append("æ·»åŠ äº†å…³æ³¨å¼•å¯¼")
        
        # 2. æ·»åŠ äº’åŠ¨å…ƒç´ 
        if 'è¯„è®º' not in optimized:
            optimized += "\n\nğŸ’¬ åœ¨è¯„è®ºåŒºåˆ†äº«ä½ çš„çœ‹æ³•å§ï½"
            changes.append("æ·»åŠ äº†è¯„è®ºäº’åŠ¨")
        
        # 3. æ§åˆ¶é•¿åº¦
        if len(optimized) > 4000:
            # å¦‚æœå¤ªé•¿ï¼Œæ·»åŠ "ç‚¹å‡»é˜…è¯»åŸæ–‡"æç¤º
            optimized = optimized[:3500] + "...\n\nğŸ‘‰ ç‚¹å‡»é˜…è¯»åŸæ–‡æŸ¥çœ‹å®Œæ•´å†…å®¹"
            changes.append("æ·»åŠ äº†é˜…è¯»åŸæ–‡å¼•å¯¼")
        
        return {
            'optimized_content': optimized,
            'changes': changes,
            'platform_score': self._calculate_platform_score(optimized, 'wechat')
        }
    
    def _optimize_for_weibo(self, content: str) -> Dict:
        """å¾®åšå†…å®¹ä¼˜åŒ–"""
        optimized = content
        changes = []
        
        # å¾®åšæœ‰å­—æ•°é™åˆ¶
        if len(optimized) > 2000:
            optimized = optimized[:1900] + "...å±•å¼€å…¨æ–‡"
            changes.append("é€‚é…å¾®åšå­—æ•°é™åˆ¶")
        
        # æ·»åŠ è¯é¢˜æ ‡ç­¾
        tags = self.generate_tags(content, 3)
        if tags:
            hashtags = ' '.join([f"#{tag}#" for tag in tags[:2]])
            optimized = f"{optimized}\n\n{hashtags}"
            changes.append("æ·»åŠ äº†è¯é¢˜æ ‡ç­¾")
        
        return {
            'optimized_content': optimized,
            'changes': changes,
            'platform_score': self._calculate_platform_score(optimized, 'weibo')
        }
    
    def _optimize_for_zhihu(self, content: str) -> Dict:
        """çŸ¥ä¹å†…å®¹ä¼˜åŒ–"""
        optimized = content
        changes = []
        
        # çŸ¥ä¹æ›´æ³¨é‡æ·±åº¦å’Œä¸“ä¸šæ€§
        if len(optimized) < 800:
            optimized += "\n\n---\n\n**æ‰©å±•é˜…è¯»å»ºè®®**ï¼š\n1. æ·±å…¥äº†è§£ç›¸å…³ç†è®º\n2. æŸ¥çœ‹æ›´å¤šå®è·µæ¡ˆä¾‹\n3. å…³æ³¨è¡Œä¸šæœ€æ–°åŠ¨æ€"
            changes.append("æ·»åŠ äº†æ‰©å±•é˜…è¯»å»ºè®®")
        
        # æ·»åŠ æ•°æ®æ”¯æ’‘
        if 'æ•°æ®' not in optimized and 'ç»Ÿè®¡' not in optimized:
            optimized += "\n\nğŸ“Š **ç›¸å…³æ•°æ®**ï¼šå»ºè®®è¡¥å……æƒå¨æ•°æ®æ”¯æ’‘è§‚ç‚¹"
            changes.append("æé†’æ·»åŠ æ•°æ®æ”¯æ’‘")
        
        return {
            'optimized_content': optimized,
            'changes': changes,
            'platform_score': self._calculate_platform_score(optimized, 'zhihu')
        }
    
    def _calculate_platform_score(self, content: str, platform: str) -> int:
        """è®¡ç®—å¹³å°é€‚é…åˆ†æ•°"""
        score = 50
        
        if platform == 'wechat':
            # å¾®ä¿¡å…¬ä¼—å·è¯„åˆ†æ ‡å‡†
            if 'å…³æ³¨' in content: score += 10
            if 'ç‚¹èµ' in content: score += 10
            if 'è¯„è®º' in content: score += 10
            if 500 <= len(content) <= 3000: score += 20
            
        elif platform == 'weibo':
            # å¾®åšè¯„åˆ†æ ‡å‡†
            if '#' in content: score += 15  # è¯é¢˜æ ‡ç­¾
            if len(content) <= 2000: score += 20  # å­—æ•°é€‚ä¸­
            if 'è½¬å‘' in content: score += 10
            
        elif platform == 'zhihu':
            # çŸ¥ä¹è¯„åˆ†æ ‡å‡†
            if len(content) >= 800: score += 20  # å†…å®¹æ·±åº¦
            if 'æ•°æ®' in content or 'ç ”ç©¶' in content: score += 15
            if 'å‚è€ƒ' in content or 'æ¥æº' in content: score += 10
        
        return min(100, score)